{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8261461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import torch\n",
    "from datasets import load_dataset, Image, DatasetDict\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomResizedCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    ToTensor,\n",
    "    Resize\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cf771e",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d414f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = 'microsoft/resnet-50'\n",
    "\n",
    "LEARNING_RATE = 3e-5 # ViTs often benefit from smaller LRs\n",
    "BATCH_SIZE = 16 # Adjust based on your GPU memory\n",
    "NUM_EPOCHS = 5 # Start with a few epochs, increase if needed\n",
    "WEIGHT_DECAY = 0.01\n",
    "REMOVE_UNUSED_COLUMNS = False # Important for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b56a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "DATASET_PATH = 'c:/users/cauchepy/Datasets/ComputerVisionImages/kaggle_terraintypes'\n",
    "SAVE_NAME = MODEL_CHECKPOINT.split(\"/\")[-1] + \"/kaggle_terraintypes\"\n",
    "OUTPUT_DIR = f\"./{SAVE_NAME}_results\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b7e82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b1a316600d4a7690febe4d4a1ed69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/3196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Load dataset using 'imagefolder'\n",
    "full_dataset = load_dataset(\"imagefolder\", data_dir=DATASET_PATH, split=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc97169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up preprocessing...\n",
      "Found classes: ['Desert', 'Forest', 'Mountain', 'Plains']\n",
      "Number of classes: 4\n",
      "Applying transformations...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Preprocessing ---\n",
    "print(\"Setting up preprocessing...\")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "labels = full_dataset.features[\"label\"].names\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "num_classes = len(labels)\n",
    "print(f\"Found classes: {labels}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(image_processor.size[\"shortest_edge\"]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        # torch.nn.Upsample(size=(image_processor.size[\"height\"], image_processor.size[\"width\"]), mode='bilinear', align_corners=False), \n",
    "        Resize(image_processor.size[\"shortest_edge\"]), # Alternative resize\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_transforms(examples):\n",
    "    # Assumes input column is named \"image\" by imagefolder loader\n",
    "    examples['pixel_values'] = [_train_transforms(img.convert(\"RGB\")) for img in examples['image']]\n",
    "    # examples.pop(\"image\", None) # Let Trainer handle column removal if needed\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(img.convert(\"RGB\")) for img in examples['image']]\n",
    "    # examples.pop(\"image\", None)\n",
    "    return examples\n",
    "\n",
    "train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42) # stratified by default\n",
    "dataset_splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "print(\"Applying transformations...\")\n",
    "dataset_splits[\"train\"].set_transform(train_transforms)\n",
    "dataset_splits[\"test\"].set_transform(val_transforms)\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # Stack tensors correctly\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples], dtype=torch.long)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7956e91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1513c43bd49b4b6f8b791f8315b02c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e85055aa7b4c7683e1dc7dfec76a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([4, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Model ---\n",
    "print(\"Loading pre-trained model...\")\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=num_classes, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True, # Allow replacing the classifier head\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21602d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up metrics...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Metrics ---\n",
    "print(\"Setting up metrics...\")\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\") \n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "# cm = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    # confusion_matrix = cm.compute(predictions=predictions, references=labels, labels=list(range(num_classes)))\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score[\"accuracy\"],\n",
    "        \"f1\": f1_score[\"f1\"],\n",
    "        \"precision\": precision_score[\"precision\"],\n",
    "        \"recall\": recall_score[\"recall\"],\n",
    "        # \"confusion_matrix\": confusion_matrix,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca10811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring training...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cauchepy\\Code\\.venv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\cauchepy\\AppData\\Local\\Temp\\ipykernel_12312\\3476970197.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57de6fdc7e54a578e8df53d961e510b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8362, 'grad_norm': 7.614301681518555, 'learning_rate': 2.8125e-05, 'epoch': 0.31}\n",
      "{'loss': 0.7872, 'grad_norm': 14.394990921020508, 'learning_rate': 2.625e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7146, 'grad_norm': 7.9564409255981445, 'learning_rate': 2.4375e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3f1844fb814d1da24bf9e6c06f09b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6522883176803589, 'eval_accuracy': 0.915625, 'eval_f1': 0.9154366351120593, 'eval_precision': 0.9165139398298544, 'eval_recall': 0.915625, 'eval_runtime': 15.7955, 'eval_samples_per_second': 40.518, 'eval_steps_per_second': 1.266, 'epoch': 1.0}\n",
      "{'loss': 0.7028, 'grad_norm': 3.9208803176879883, 'learning_rate': 2.25e-05, 'epoch': 1.25}\n",
      "{'loss': 0.6621, 'grad_norm': 6.2772393226623535, 'learning_rate': 2.0625e-05, 'epoch': 1.56}\n",
      "{'loss': 0.5928, 'grad_norm': 11.927155494689941, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45877b80a2b24c20a4c577d13dee594d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4445533752441406, 'eval_accuracy': 0.934375, 'eval_f1': 0.9340242126956617, 'eval_precision': 0.9352137927499452, 'eval_recall': 0.934375, 'eval_runtime': 15.9429, 'eval_samples_per_second': 40.143, 'eval_steps_per_second': 1.254, 'epoch': 2.0}\n",
      "{'loss': 0.539, 'grad_norm': 4.298440456390381, 'learning_rate': 1.6875e-05, 'epoch': 2.19}\n",
      "{'loss': 0.5172, 'grad_norm': 5.234574794769287, 'learning_rate': 1.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.4719, 'grad_norm': 9.046529769897461, 'learning_rate': 1.3125e-05, 'epoch': 2.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16779223e2a946cc97a5bd8cab5b8894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34848150610923767, 'eval_accuracy': 0.9359375, 'eval_f1': 0.9356858626251565, 'eval_precision': 0.9369410387222068, 'eval_recall': 0.9359375, 'eval_runtime': 16.2868, 'eval_samples_per_second': 39.296, 'eval_steps_per_second': 1.228, 'epoch': 3.0}\n",
      "{'loss': 0.4577, 'grad_norm': 5.474575042724609, 'learning_rate': 1.125e-05, 'epoch': 3.12}\n",
      "{'loss': 0.5051, 'grad_norm': 7.038826942443848, 'learning_rate': 9.375000000000001e-06, 'epoch': 3.44}\n",
      "{'loss': 0.4194, 'grad_norm': 3.5866682529449463, 'learning_rate': 7.5e-06, 'epoch': 3.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8774f255aff44518c2d279abc919a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3036420941352844, 'eval_accuracy': 0.9453125, 'eval_f1': 0.9451613553734978, 'eval_precision': 0.9461135704279812, 'eval_recall': 0.9453125, 'eval_runtime': 15.9381, 'eval_samples_per_second': 40.155, 'eval_steps_per_second': 1.255, 'epoch': 4.0}\n",
      "{'loss': 0.4236, 'grad_norm': 8.337488174438477, 'learning_rate': 5.625e-06, 'epoch': 4.06}\n",
      "{'loss': 0.4247, 'grad_norm': 5.168450355529785, 'learning_rate': 3.75e-06, 'epoch': 4.38}\n",
      "{'loss': 0.4354, 'grad_norm': 7.3479533195495605, 'learning_rate': 1.875e-06, 'epoch': 4.69}\n",
      "{'loss': 0.4293, 'grad_norm': 3.8558385372161865, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406219a0d82d423f8081a7d9c2a21ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2705650329589844, 'eval_accuracy': 0.953125, 'eval_f1': 0.9529610527965791, 'eval_precision': 0.9541800211897445, 'eval_recall': 0.953125, 'eval_runtime': 16.3243, 'eval_samples_per_second': 39.205, 'eval_steps_per_second': 1.225, 'epoch': 5.0}\n",
      "{'train_runtime': 1125.5211, 'train_samples_per_second': 11.355, 'train_steps_per_second': 0.711, 'train_loss': 0.5574478054046631, 'epoch': 5.0}\n",
      "Training finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./resnet-50/kaggle_terraintypes_processor\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 5. Training ---\n",
    "print(\"Configuring training...\")\n",
    "\n",
    "# Define Training Arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # Training Hyperparameters\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2, # Usually can use larger batch for eval\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    # Evaluation and Saving\n",
    "    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",      # Save model checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True, # Load the best model found during training\n",
    "    metric_for_best_model=\"accuracy\", # Use accuracy to determine the best model\n",
    "    # Technical settings\n",
    "    logging_dir=f'{OUTPUT_DIR}/logs',\n",
    "    logging_steps=50, # Log training loss every N steps\n",
    "    remove_unused_columns=REMOVE_UNUSED_COLUMNS, # Keep necessary columns like 'label'\n",
    "    push_to_hub=False, # Set to True to upload model to Hugging Face Hub\n",
    "    report_to=\"tensorboard\", # Or \"wandb\" if you use Weights & Biases\n",
    "    seed=42, # For reproducibility\n",
    ")\n",
    "\n",
    "# Instantiate Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_splits[\"train\"],\n",
    "    eval_dataset=dataset_splits[\"test\"], # Using test split as validation here\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=image_processor, # Pass processor for consistent saving\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"Starting training...\")\n",
    "train_results = trainer.train()\n",
    "\n",
    "# Save the final best model and processor\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n",
    "image_processor.save_pretrained(OUTPUT_DIR) # Save processor alongside model\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "model.save_pretrained(f\"./{SAVE_NAME}_model\")\n",
    "image_processor.save_pretrained(f\"./{SAVE_NAME}_processor\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f61c8af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9777fb6b25994d319774fffe04b060e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.2705650329589844, 'eval_accuracy': 0.953125, 'eval_f1': 0.9529610527965791, 'eval_precision': 0.9541800211897445, 'eval_recall': 0.953125, 'eval_runtime': 17.9144, 'eval_samples_per_second': 35.725, 'eval_steps_per_second': 1.116, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Evaluation ---\n",
    "print(\"Evaluating model on the test set...\")\n",
    "eval_results = trainer.evaluate(dataset_splits[\"test\"])\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93039c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Prediction ---\n",
      "Loading example image: <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x768 at 0x25189B4C8B0>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "read",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# image_to_predict = dataset_splits[\"test\"][0]['image'].filename\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading example image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_to_predict\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mPILImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_to_predict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m image_processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\cauchepy\\Code\\.venv\\lib\\site-packages\\PIL\\Image.py:3480\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3477\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m   3478\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3480\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m   3482\u001b[0m preinit()\n\u001b[0;32m   3484\u001b[0m warning_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\cauchepy\\Code\\.venv\\lib\\site-packages\\PIL\\JpegImagePlugin.py:396\u001b[0m, in \u001b[0;36mJpegImageFile.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    394\u001b[0m     deprecate(name, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name)\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: read"
     ]
    }
   ],
   "source": [
    "# --- 7. Prediction ---\n",
    "\n",
    "print(\"\\n--- Example Prediction ---\")\n",
    "from PIL import Image as PILImage\n",
    "import requests\n",
    "\n",
    "# Example: Load an image (replace with your own image path/URL)\n",
    "# url = \"https://wallpapercave.com/wp/JZQsFFO.jpg\"\n",
    "# image_to_predict = PILImage.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image_to_predict = dataset_splits[\"test\"][0]['image'].filename\n",
    "\n",
    "print(f\"Loading example image: {image_to_predict}\")\n",
    "image = PILImage.open(image_to_predict)\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "inputs = {k: v.to(trainer.model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "predicted_class_label = model.config.id2label[predicted_class_idx]\n",
    "\n",
    "print(f\"Predicted class: {predicted_class_label} (Index: {predicted_class_idx})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b52e2188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133c24373b1e481fb99c082987c5e039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c79fd364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1b0a6a085a4b3982faebc5ab444432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/94.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yanncauchepin/kaggle_terraintypes_resnet_processor/commit/2017fb4359d860a4af8bb2c7c114e91f0a9f52f4', commit_message='Upload processor', commit_description='', oid='2017fb4359d860a4af8bb2c7c114e91f0a9f52f4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yanncauchepin/kaggle_terraintypes_resnet_processor', endpoint='https://huggingface.co', repo_type='model', repo_id='yanncauchepin/kaggle_terraintypes_resnet_processor'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_repo_id = \"yanncauchepin/kaggle_terraintypes_resnet_model\"\n",
    "model.push_to_hub(model_repo_id)\n",
    "\n",
    "processor_repo_id = \"yanncauchepin/kaggle_terraintypes_resnet_processor\"\n",
    "image_processor.push_to_hub(processor_repo_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8261461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cauchepy\\Code\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Import Libraries ---\n",
    "import torch\n",
    "from datasets import load_dataset, Image, DatasetDict\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomResizedCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    ToTensor,\n",
    "    Resize\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325e5d6",
   "metadata": {},
   "source": [
    "# ConvNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeaa754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = 'facebook/convnext-tiny-224'\n",
    "MODEL_CHECKPOINT = 'facebook/convnext-base-224'\n",
    "\n",
    "LEARNING_RATE = 3e-5 # ViTs often benefit from smaller LRs\n",
    "BATCH_SIZE = 16 # Adjust based on your GPU memory\n",
    "NUM_EPOCHS = 5 # Start with a few epochs, increase if needed\n",
    "WEIGHT_DECAY = 0.01\n",
    "REMOVE_UNUSED_COLUMNS = False # Important for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b56a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "DATASET_PATH = 'c:/users/cauchepy/Datasets/ComputerVisionImages/kaggle_terraintypes'\n",
    "SAVE_NAME = MODEL_CHECKPOINT.split(\"/\")[-1] + \"/kaggle_terraintypes\"\n",
    "OUTPUT_DIR = f\"./{SAVE_NAME}_results\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5b7e82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465b2d4c0b6b40adba90fabad865f9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/3196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Load dataset using 'imagefolder'\n",
    "full_dataset = load_dataset(\"imagefolder\", data_dir=DATASET_PATH, split=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc97169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up preprocessing...\n",
      "Found classes: ['Desert', 'Forest', 'Mountain', 'Plains']\n",
      "Number of classes: 4\n",
      "Applying transformations...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Preprocessing ---\n",
    "print(\"Setting up preprocessing...\")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "labels = full_dataset.features[\"label\"].names\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "num_classes = len(labels)\n",
    "print(f\"Found classes: {labels}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(image_processor.size[\"shortest_edge\"]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        # torch.nn.Upsample(size=(image_processor.size[\"height\"], image_processor.size[\"width\"]), mode='bilinear', align_corners=False), \n",
    "        Resize(image_processor.size[\"shortest_edge\"]), # Alternative resize\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_transforms(examples):\n",
    "    # Assumes input column is named \"image\" by imagefolder loader\n",
    "    examples['pixel_values'] = [_train_transforms(img.convert(\"RGB\")) for img in examples['image']]\n",
    "    # examples.pop(\"image\", None) # Let Trainer handle column removal if needed\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(img.convert(\"RGB\")) for img in examples['image']]\n",
    "    # examples.pop(\"image\", None)\n",
    "    return examples\n",
    "\n",
    "train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42) # stratified by default\n",
    "dataset_splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "print(\"Applying transformations...\")\n",
    "dataset_splits[\"train\"].set_transform(train_transforms)\n",
    "dataset_splits[\"test\"].set_transform(val_transforms)\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # Stack tensors correctly\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples], dtype=torch.long)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7956e91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f9944eea18473eb0da8d891fabfea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9582a429b577427495323a5f25eb4fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/354M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at facebook/convnext-base-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([4, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Model ---\n",
    "print(\"Loading pre-trained model...\")\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=num_classes, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True, # Allow replacing the classifier head\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21602d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up metrics...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Metrics ---\n",
    "print(\"Setting up metrics...\")\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\") \n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "cm = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    # confusion_matrix = cm.compute(predictions=predictions, references=labels, labels=list(range(num_classes)))\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score[\"accuracy\"],\n",
    "        \"f1\": f1_score[\"f1\"],\n",
    "        \"precision\": precision_score[\"precision\"],\n",
    "        \"recall\": recall_score[\"recall\"],\n",
    "        # \"confusion_matrix\": confusion_matrix,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca10811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring training...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cauchepy\\Code\\.venv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\cauchepy\\AppData\\Local\\Temp\\ipykernel_6012\\3476970197.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812ba2f27f4543b39fa27e1faaede942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9942, 'grad_norm': 3.970083475112915, 'learning_rate': 2.8125e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3065, 'grad_norm': 5.2944464683532715, 'learning_rate': 2.625e-05, 'epoch': 0.62}\n",
      "{'loss': 0.149, 'grad_norm': 1.3110170364379883, 'learning_rate': 2.4375e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9187b37854604fe59ab2f3800a9519b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06476698815822601, 'eval_accuracy': 0.9859375, 'eval_f1': 0.9859317442434818, 'eval_precision': 0.9860517559085729, 'eval_recall': 0.9859375, 'eval_runtime': 44.3046, 'eval_samples_per_second': 14.445, 'eval_steps_per_second': 0.451, 'epoch': 1.0}\n",
      "{'loss': 0.1002, 'grad_norm': 3.213397264480591, 'learning_rate': 2.25e-05, 'epoch': 1.25}\n",
      "{'loss': 0.1122, 'grad_norm': 0.10792824625968933, 'learning_rate': 2.0625e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0638, 'grad_norm': 0.10048019140958786, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a93c500c33443ea2865131a4f62cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04316555708646774, 'eval_accuracy': 0.9921875, 'eval_f1': 0.9921857774660395, 'eval_precision': 0.9922326585707111, 'eval_recall': 0.9921875, 'eval_runtime': 44.8674, 'eval_samples_per_second': 14.264, 'eval_steps_per_second': 0.446, 'epoch': 2.0}\n",
      "{'loss': 0.0871, 'grad_norm': 0.8476230502128601, 'learning_rate': 1.6875e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0648, 'grad_norm': 0.24083544313907623, 'learning_rate': 1.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.0589, 'grad_norm': 28.654264450073242, 'learning_rate': 1.3125e-05, 'epoch': 2.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab846274e834d8f8fcede3b8c98a049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.037061549723148346, 'eval_accuracy': 0.9921875, 'eval_f1': 0.9921868838783136, 'eval_precision': 0.9921961325966852, 'eval_recall': 0.9921875, 'eval_runtime': 46.7645, 'eval_samples_per_second': 13.686, 'eval_steps_per_second': 0.428, 'epoch': 3.0}\n",
      "{'loss': 0.0709, 'grad_norm': 4.003460884094238, 'learning_rate': 1.125e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0719, 'grad_norm': 22.93167495727539, 'learning_rate': 9.375000000000001e-06, 'epoch': 3.44}\n",
      "{'loss': 0.0611, 'grad_norm': 0.06833165138959885, 'learning_rate': 7.5e-06, 'epoch': 3.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad41d493422947d0926bd635c2c2b862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.034258898347616196, 'eval_accuracy': 0.9921875, 'eval_f1': 0.9921868838783136, 'eval_precision': 0.9921961325966852, 'eval_recall': 0.9921875, 'eval_runtime': 44.9762, 'eval_samples_per_second': 14.23, 'eval_steps_per_second': 0.445, 'epoch': 4.0}\n",
      "{'loss': 0.047, 'grad_norm': 0.10565850138664246, 'learning_rate': 5.625e-06, 'epoch': 4.06}\n",
      "{'loss': 0.0345, 'grad_norm': 0.051449015736579895, 'learning_rate': 3.75e-06, 'epoch': 4.38}\n",
      "{'loss': 0.061, 'grad_norm': 0.45013824105262756, 'learning_rate': 1.875e-06, 'epoch': 4.69}\n",
      "{'loss': 0.0528, 'grad_norm': 0.0368526466190815, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3e537c5ebc4b929110017ebe89cbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.039769165217876434, 'eval_accuracy': 0.9890625, 'eval_f1': 0.989057825436755, 'eval_precision': 0.9892171399241356, 'eval_recall': 0.9890625, 'eval_runtime': 57.0095, 'eval_samples_per_second': 11.226, 'eval_steps_per_second': 0.351, 'epoch': 5.0}\n",
      "{'train_runtime': 3503.3004, 'train_samples_per_second': 3.648, 'train_steps_per_second': 0.228, 'train_loss': 0.14597934991121292, 'epoch': 5.0}\n",
      "Training finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./convnext-base-224/kaggle_terraintypes_processor\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 5. Training ---\n",
    "print(\"Configuring training...\")\n",
    "\n",
    "# Define Training Arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # Training Hyperparameters\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2, # Usually can use larger batch for eval\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    # Evaluation and Saving\n",
    "    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",      # Save model checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True, # Load the best model found during training\n",
    "    metric_for_best_model=\"accuracy\", # Use accuracy to determine the best model\n",
    "    # Technical settings\n",
    "    logging_dir=f'{OUTPUT_DIR}/logs',\n",
    "    logging_steps=50, # Log training loss every N steps\n",
    "    remove_unused_columns=REMOVE_UNUSED_COLUMNS, # Keep necessary columns like 'label'\n",
    "    push_to_hub=False, # Set to True to upload model to Hugging Face Hub\n",
    "    report_to=\"tensorboard\", # Or \"wandb\" if you use Weights & Biases\n",
    "    seed=42, # For reproducibility\n",
    ")\n",
    "\n",
    "# Instantiate Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_splits[\"train\"],\n",
    "    eval_dataset=dataset_splits[\"test\"], # Using test split as validation here\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=image_processor, # Pass processor for consistent saving\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"Starting training...\")\n",
    "train_results = trainer.train()\n",
    "\n",
    "# Save the final best model and processor\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n",
    "image_processor.save_pretrained(OUTPUT_DIR) # Save processor alongside model\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "model.save_pretrained(f\"./{SAVE_NAME}_model\")\n",
    "image_processor.save_pretrained(f\"./{SAVE_NAME}_processor\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f61c8af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d20b17b0ab4eb39874eb38ce777913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.04316555708646774, 'eval_accuracy': 0.9921875, 'eval_f1': 0.9921857774660395, 'eval_precision': 0.9922326585707111, 'eval_recall': 0.9921875, 'eval_runtime': 48.8379, 'eval_samples_per_second': 13.105, 'eval_steps_per_second': 0.41, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Evaluation ---\n",
    "print(\"Evaluating model on the test set...\")\n",
    "eval_results = trainer.evaluate(dataset_splits[\"test\"])\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93039c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Prediction ---\n",
      "Loading example image: <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x768 at 0x25189B4C8B0>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "read",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# image_to_predict = dataset_splits[\"test\"][0]['image'].filename\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading example image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_to_predict\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mPILImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_to_predict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m image_processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\cauchepy\\Code\\.venv\\lib\\site-packages\\PIL\\Image.py:3480\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3477\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m   3478\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3480\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m   3482\u001b[0m preinit()\n\u001b[0;32m   3484\u001b[0m warning_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\cauchepy\\Code\\.venv\\lib\\site-packages\\PIL\\JpegImagePlugin.py:396\u001b[0m, in \u001b[0;36mJpegImageFile.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    394\u001b[0m     deprecate(name, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name)\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: read"
     ]
    }
   ],
   "source": [
    "# --- 7. Prediction ---\n",
    "\n",
    "print(\"\\n--- Example Prediction ---\")\n",
    "from PIL import Image as PILImage\n",
    "import requests\n",
    "\n",
    "# Example: Load an image (replace with your own image path/URL)\n",
    "# url = \"https://wallpapercave.com/wp/JZQsFFO.jpg\"\n",
    "# image_to_predict = PILImage.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# image_to_predict = dataset_splits[\"test\"][0]['image'].filename\n",
    "\n",
    "print(f\"Loading example image: {image_to_predict}\")\n",
    "image = PILImage.open(image_to_predict)\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "inputs = {k: v.to(trainer.model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "predicted_class_label = model.config.id2label[predicted_class_idx]\n",
    "\n",
    "print(f\"Predicted class: {predicted_class_label} (Index: {predicted_class_idx})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b52e2188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a0986e411441baa7e04fbc4d7f4854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79fd364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04331fb596ba4180a576f34d511295d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/350M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yanncauchepin/kaggle_terraintypes_convnext_processor/commit/6976d5a3a3ccc5da64f9102a4dc212cf412575d8', commit_message='Upload processor', commit_description='', oid='6976d5a3a3ccc5da64f9102a4dc212cf412575d8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yanncauchepin/kaggle_terraintypes_convnext_processor', endpoint='https://huggingface.co', repo_type='model', repo_id='yanncauchepin/kaggle_terraintypes_convnext_processor'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_repo_id = \"yanncauchepin/kaggle_terraintypes_convnext_model\"\n",
    "model.push_to_hub(model_repo_id)\n",
    "\n",
    "processor_repo_id = \"yanncauchepin/kaggle_terraintypes_convnext_processor\"\n",
    "image_processor.push_to_hub(processor_repo_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
